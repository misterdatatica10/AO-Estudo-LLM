# Assistente de Estudo com Ollama

![Assistente de Estudo](Imagem.PNG)

Uma aplicaÃ§Ã£o que utiliza modelos de linguagem local (via Ollama) para auxiliar estudantes nos estudos. Responde a perguntas, gera resumos e cria planos de estudo.

## Funcionalidades

- ğŸ’¡ Respostas a perguntas sobre matÃ©rias especÃ­ficas
- ğŸ“ Gera resumos
- ğŸ“š RevisÃ£o de conteÃºdo acadÃ©mico
- ğŸ¯ SugestÃµes de estudo personalizadas

## Requisitos

- Python 3.11+
- Ollama instalado e em execuÃ§Ã£o local
- Modelo Llama2 ou Mistral descarregado no Ollama

## InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone [URL_DO_REPOSITÃ“RIO]
cd [NOME_DO_DIRETÃ“RIO]
```

2. Crie e ative o ambiente virtual:
```bash
python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Instale o Ollama:
- Windows: [InstruÃ§Ãµes de instalaÃ§Ã£o do Ollama](https://github.com/ollama/ollama)
- Linux/Mac: 
```bash
curl https://ollama.ai/install.sh | sh
```

5. Descarregue o modelo Llama2:
```bash
ollama pull llama2
```

## ExecuÃ§Ã£o

1. Inicie o Ollama:
```bash
ollama serve
```

2. Noutro terminal, inicie a aplicaÃ§Ã£o:
```bash
streamlit run app.py
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em `http://localhost:8501`

## Estrutura do Projeto

```
.
â”œâ”€â”€ app.py                 # AplicaÃ§Ã£o principal Streamlit
â”œâ”€â”€ backend/              
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ollama_client.py   # Interface com Ollama
â”‚   â””â”€â”€ study_utils.py     # FunÃ§Ãµes de processamento de estudo
â”œâ”€â”€ utils/               
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py        # FunÃ§Ãµes auxiliares
â”œâ”€â”€ static/              # Ficheiros estÃ¡ticos
â”œâ”€â”€ requirements.txt     # DependÃªncias
â””â”€â”€ README.md           # DocumentaÃ§Ã£o
```

## Tecnologias Utilizadas

- [Streamlit](https://github.com/streamlit/streamlit) - Framework para desenvolvimento rÃ¡pido de aplicaÃ§Ãµes web
- [Ollama](https://github.com/ollama/ollama) - Framework para executar LLMs localmente
- [Llama2](https://github.com/facebookresearch/llama) - Modelo de linguagem open-source